{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==1.0.0rc0\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/2f/91d5e0fa358e2568acea1941d09547ce5cff32f46973cefe499261b0c3c9/pandas-1.0.0rc0-cp36-cp36m-manylinux1_x86_64.whl (10.0MB)\r\n",
      "\u001b[K     |████████████████████████████████| 10.0MB 2.4MB/s \r\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas==1.0.0rc0) (2019.3)\r\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /opt/conda/lib/python3.6/site-packages (from pandas==1.0.0rc0) (1.18.1)\r\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from pandas==1.0.0rc0) (2.8.1)\r\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas==1.0.0rc0) (1.13.0)\r\n",
      "\u001b[31mERROR: tpot 0.11.1 has requirement scikit-learn>=0.22.0, but you'll have scikit-learn 0.21.3 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: mizani 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.0.3 which is incompatible.\u001b[0m\r\n",
      "Installing collected packages: pandas\r\n",
      "  Found existing installation: pandas 0.25.3\r\n",
      "    Uninstalling pandas-0.25.3:\r\n",
      "      Successfully uninstalled pandas-0.25.3\r\n",
      "Successfully installed pandas-1.0.0rc0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pandas==1.0.0rc0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas Version 1.0.0rc0\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "print('Pandas Version {}'.format(pd.__version__))\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Dropout\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Set Shape = (5572, 2)\n",
      "Data Set Memory Usage = 0.09 MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../input/spam-text-message-classification/SPAM text message 20170820 - Data.csv')\n",
    "\n",
    "print('Data Set Shape = {}'.format(df.shape))\n",
    "print('Data Set Memory Usage = {:.2f} MB'.format(df.memory_usage().sum() / 1024**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GloVe Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.1 s, sys: 530 ms, total: 21.6 s\n",
      "Wall time: 21.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "glove_embeddings = {}\n",
    "\n",
    "with open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        glove_embeddings[word] = np.asarray(values[1:], 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings cover 31.44% of vocab\n",
      "Embeddings cover 64.08% of text\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(df):\n",
    "    \n",
    "    text = df['Message'].apply(lambda s: s.split()).values      \n",
    "    vocab = {}\n",
    "    \n",
    "    for message in text:\n",
    "        for word in message:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1                \n",
    "    return vocab\n",
    "\n",
    "\n",
    "def check_embedding_coverage(df, embeddings):\n",
    "    \n",
    "    vocab = build_vocab(df)    \n",
    "    \n",
    "    covered = {}\n",
    "    oov = {}    \n",
    "    n_covered = 0\n",
    "    n_oov = 0\n",
    "    \n",
    "    for word in vocab:\n",
    "        try:\n",
    "            covered[word] = embeddings[word]\n",
    "            n_covered += vocab[word]\n",
    "        except:\n",
    "            oov[word] = vocab[word]\n",
    "            n_oov += vocab[word]\n",
    "            \n",
    "    vocab_coverage = len(covered) / len(vocab)\n",
    "    text_coverage = (n_covered / (n_covered + n_oov))\n",
    "    print('Embeddings cover {:.2%} of vocab'.format(vocab_coverage))\n",
    "    print('Embeddings cover {:.2%} of text'.format(text_coverage))\n",
    "    \n",
    "    sorted_oov = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "    return sorted_oov\n",
    "\n",
    "\n",
    "oov = check_embedding_coverage(df, glove_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Text Pre-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings cover 75.19% of vocab\n",
      "Embeddings cover 96.56% of text\n"
     ]
    }
   ],
   "source": [
    "def clean(text):\n",
    "    \n",
    "    # Contractions\n",
    "    text = re.sub(r\"Its\", \"It is\", text)\n",
    "    text = re.sub(r\"i'm\", \"I am\", text)\n",
    "    text = re.sub(r\"i'll\", \"I will\", text)\n",
    "    text = re.sub(r\"That's\", \"That is\", text)\n",
    "    text = re.sub(r\"i've\", \"I have\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"I.ll\", \"I will\", text)\n",
    "    text = re.sub(r\"haven't\", \"have not\", text)\n",
    "    text = re.sub(r\"there's\", \"there is\", text)\n",
    "    text = re.sub(r\"we're\", \"we are\", text)\n",
    "    text = re.sub(r\"we'll\", \"we will\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"What's\", \"What is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"How's\", \"How is\", text)\n",
    "    text = re.sub(r\"wasn't\", \"was not\", text)\n",
    "    text = re.sub(r\"i.ll\", \"I will\", text)\n",
    "    text = re.sub(r\"isn't\", \"is not\", text)\n",
    "    text = re.sub(r\"You're\", \"You are\", text)\n",
    "    text = re.sub(r\"you've\", \"you have\", text)\n",
    "    text = re.sub(r\"You've\", \"You have\", text)\n",
    "    text = re.sub(r\"i'd\", \"I would\", text)\n",
    "    text = re.sub(r\"We're\", \"We are\", text)\n",
    "    text = re.sub(r\"you'd\", \"you would\", text)\n",
    "    text = re.sub(r\"Haven't\", \"Have not\", text)\n",
    "    text = re.sub(r\"She.s\", \"She is\", text)\n",
    "    text = re.sub(r\"did'nt\", \"did not\", text)\n",
    "    text = re.sub(r\"Wat's\", \"What is\", text)\n",
    "    text = re.sub(r\"she.s\", \"she is\", text)\n",
    "    text = re.sub(r\"couldn't\", \"could not\", text)\n",
    "    text = re.sub(r\"u're\", \"you are\", text)\n",
    "    text = re.sub(r\"Can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"did't\", \"did not\", text)\n",
    "    text = re.sub(r\"he'll\", \"he will\", text)\n",
    "    text = re.sub(r\"We'll\", \"We will\", text)\n",
    "    text = re.sub(r\"aren't\", \"are not\", text)\n",
    "    text = re.sub(r\"shouldn't\", \"should not\", text)\n",
    "    text = re.sub(r\"wouldn't\", \"would not\", text)\n",
    "    text = re.sub(r\"Who's\", \"Who is\", text)\n",
    "    text = re.sub(r\"don‘t\", \"do not\", text)\n",
    "    text = re.sub(r\"let's\", \"let us\", text)\n",
    "    text = re.sub(r\"ain't\", \"am not\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"You'll\", \"You will\", text)\n",
    "    text = re.sub(r\"I‘m\", \"I am\", text)\n",
    "    text = re.sub(r\"dsn't\", \"does not\", text)\n",
    "    text = re.sub(r\"THERE'S\", \"THERE IS\", text)\n",
    "    text = re.sub(r\"cann't\", \"cannot\", text)\n",
    "    text = re.sub(r\"who's\", \"who is\", text)\n",
    "    text = re.sub(r\"There's\", \"There is\", text)\n",
    "    text = re.sub(r\"There's\", \"There is\", text)\n",
    "    text = re.sub(r\"He's\", \"He is\", text)\n",
    "    text = re.sub(r\"they're\", \"they are\", text)\n",
    "    text = re.sub(r\"U've\", \"You have\", text)\n",
    "    text = re.sub(r\"He's\", \"He is\", text)\n",
    "    text = re.sub(r\"u'll\", \"you will\", text)\n",
    "    text = re.sub(r\"Where's\", \"Where is\", text)\n",
    "    text = re.sub(r\"DON\\x92T\", \"DO NOT\", text)\n",
    "    text = re.sub(r\"hasn't\", \"has not\", text)\n",
    "    text = re.sub(r\"i\\x92m\", \"I am\", text)\n",
    "    text = re.sub(r\"We‘re\", \"We are\", text)\n",
    "    text = re.sub(r\"We'd\", \"We would\", text)\n",
    "    text = re.sub(r\"It‘s\", \"It is\", text)\n",
    "    text = re.sub(r\"THAT\\x92S\", \"THAT IS\", text)\n",
    "    text = re.sub(r\"They're\", \"They are\", text)\n",
    "    text = re.sub(r\"we've\", \"we have\", text)\n",
    "    text = re.sub(r\"THAT\\x92S\", \"THAT IS\", text)\n",
    "    text = re.sub(r\"that‘s\", \"that is\", text)\n",
    "    text = re.sub(r\"dat's\", \"that is\", text)\n",
    "    text = re.sub(r\"didn‘t\", \"did not\", text)\n",
    "    text = re.sub(r\"I\\x92m\", \"I am\", text)\n",
    "    text = re.sub(r\"doesn\\\\\", \"does not\", text)\n",
    "    text = re.sub(r\"i\\x92d\", \"I would\", text)    \n",
    "        \n",
    "    # Character entity references\n",
    "    text = re.sub(r\"&amp;\", \"and\", text)\n",
    "    text = re.sub(r\"&gt;\", \">\", text)\n",
    "    text = re.sub(r\"&lt;\", \"<\", text)\n",
    "    \n",
    "    # Slang, typo, abbreviation\n",
    "    text = re.sub(r\"MobileUpd8\", \"Mobile Update\", text)\n",
    "    text = re.sub(r\"Aight,\", \"all right,\", text)\n",
    "    text = re.sub(r\"aight,\", \"all right,\", text)\n",
    "    text = re.sub(r\"Max10mins\", \"maximum 10 minutes\", text)\n",
    "    text = re.sub(r\"b'day\", \"birthday\", text)\n",
    "    text = re.sub(r\"Thanx\", \"Thanks\", text)\n",
    "    text = re.sub(r\"un-redeemed\", \"unredeemed\", text)\n",
    "    text = re.sub(r\"Bstfrnd\", \"best friend\", text)\n",
    "    text = re.sub(r\"Swtheart\", \"sweetheart\", text)\n",
    "    text = re.sub(r\"Belovd\", \"beloved\", text)\n",
    "    text = re.sub(r\"Lifpartnr\", \"life partner\", text)\n",
    "    text = re.sub(r\"Cutefrnd\", \"cute friend\", text)\n",
    "    text = re.sub(r\"Jstfrnd\", \"just friend\", text)   \n",
    "    text = re.sub(r\"Lvblefrnd\", \"lovable friend\", text)\n",
    "    text = re.sub(r\"Suite342\", \"Suite 342\", text)\n",
    "    text = re.sub(r\"FreeMsg\", \"Free Message\", text)\n",
    "    text = re.sub(r\"call2optout\", \"call to opt out\", text)\n",
    "    text = re.sub(r\"toClaim\", \"to claim\", text)\n",
    "    text = re.sub(r\"girlfrnd\", \"girlfriend\", text)\n",
    "    text = re.sub(r\"AfterNoon\", \"Afternoon\", text)\n",
    "    text = re.sub(r\"SkillGame\", \"skill game\", text)\n",
    "    text = re.sub(r\"ringtoneking\", \"ringtone king\", text)\n",
    "    text = re.sub(r\"invnted\", \"invented\", text)\n",
    "    text = re.sub(r\"Grahmbell\", \"Graham Bell\", text)\n",
    "    text = re.sub(r\"Whenevr\", \"Whenever\", text)\n",
    "    text = re.sub(r\"Valid12hrs\", \"Valid 12 hours\", text)\n",
    "    text = re.sub(r\"Age16\", \"Age 16\", text)\n",
    "    text = re.sub(r\"StarWars3\", \"Star Wars 3\", text)\n",
    "    text = re.sub(r\"Suprman\", \"Superman\", text)\n",
    "    text = re.sub(r\"Call2OptOut\", \"call to opt out\", text)\n",
    "    text = re.sub(r\"iscoming\", \"is coming\", text)\n",
    "    text = re.sub(r\"GOODFRIEND\", \"good friend\", text)\n",
    "    text = re.sub(r\"age23\", \"age 23\", text)\n",
    "    text = re.sub(r\"age16\", \"age 16\", text)    \n",
    "    text = re.sub(r\"b\\x92day\", \"birthday\", text)\n",
    "    text = re.sub(r\"Alwys\", \"always\", text)\n",
    "    text = re.sub(r\"LookAtMe\", \"Look at me\", text)\n",
    "    text = re.sub(r\"EURO2004\", \"Euro 2004\", text)\n",
    "    text = re.sub(r\"transfr\", \"transfer\", text)\n",
    "    text = re.sub(r\"movietrivia\", \"movie trivia\", text)\n",
    "    text = re.sub(r\"FREE2DAY\", \"free today\", text)\n",
    "    text = re.sub(r\"2optout\", \"to opt out\", text)\n",
    "    text = re.sub(r\"Callertune\", \"caller tune\", text)   \n",
    "    text = re.sub(r\"callertune\", \"caller tune\", text) \n",
    "    text = re.sub(r\"urgnt\", \"urgent\", text)   \n",
    "    text = re.sub(r\"PICSFREE1\", \"pictures free 1\", text)   \n",
    "    text = re.sub(r\"SkilGme\", \"Skill Game\", text) \n",
    "    text = re.sub(r\"bcums\", \"becomes\", text) \n",
    "    text = re.sub(r\"DeliveredTomorrow\", \"Delivered Tomorrow\", text) \n",
    "    text = re.sub(r\"2MORO\", \"Tomorrow\", text) \n",
    "    text = re.sub(r\"linerental\", \"line rental\", text) \n",
    "    text = re.sub(r\"MobilesDirect\", \"Mobiles Direct\", text) \n",
    "    text = re.sub(r\"125gift\", \"125 gift\", text) \n",
    "    text = re.sub(r\"gr8prizes\", \"great prizes\", text)   \n",
    "    text = re.sub(r\"msgs\", \"messages\", text)\n",
    "    text = re.sub(r\"12hrs\", \"12 hours\", text)\n",
    "    text = re.sub(r\"frnd\", \"friend\", text)    \n",
    "        \n",
    "    # Words with punctuations and special characters\n",
    "    punctuations = '#!,£?+&*<>()\"@%-=;' + \"'\"\n",
    "    \n",
    "    for p in punctuations:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "        \n",
    "    text = text.replace('...', ' ... ')\n",
    "    if '...' not in text:\n",
    "        text = text.replace('..', ' ... ')\n",
    "        \n",
    "    if 'www' not in text or 'http' not in text or '...' not in text or '..' not in text:\n",
    "        text = text.replace('.', ' . ')\n",
    "        text = text.replace(':', ' : ')\n",
    "        text = text.replace('/', ' / ')\n",
    "    \n",
    "    return text.lower()\n",
    "    \n",
    "# oov after text cleaning\n",
    "df['Message'] = df['Message'].apply(lambda s : clean(s))\n",
    "oov = check_embedding_coverage(df, glove_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  68,  488, 4333, ...,    0,    0,    0],\n",
       "       [  63,  360,    1, ...,    0,    0,    0],\n",
       "       [  61,  506,   14, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [8778,    5,   58, ...,    0,    0,    0],\n",
       "       [   9,  514,  123, ...,    0,    0,    0],\n",
       "       [2614,    1,   15, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 50\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "def create_corpus(df):    \n",
    "    corpus = []\n",
    "    \n",
    "    for message in df['Message']:\n",
    "        words = [word.lower() for word in word_tokenize(message)]\n",
    "        corpus.append(words)\n",
    "        \n",
    "    return corpus \n",
    "\n",
    "corpus = create_corpus(df)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "sequences2d = pad_sequences(sequences, maxlen=MAX_LEN, truncating='post', padding='post')\n",
    "sequences2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train/Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Shape = (4457, 50) (Class ham = 3859 - Class spam = 598)\n",
      "X_test Shape = (1115, 50) (Class ham = 966 - Class spam = 149)\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([df, pd.DataFrame(sequences2d)], axis=1)\n",
    "df['Category'] = df['Category'].map({'ham': 0, 'spam': 1})\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[df.columns[2:]], df['Category'], test_size=0.2, stratify=df['Category'], random_state=SEED)\n",
    "\n",
    "print('X_train Shape = {} (Class ham = {} - Class spam = {})'.format(X_train.shape, y_train[y_train == 0].shape[0], y_train[y_train == 1].shape[0]))\n",
    "print('X_test Shape = {} (Class ham = {} - Class spam = {})'.format(X_test.shape, y_test[y_test == 0].shape[0], y_test[y_test == 1].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(embeddings):    \n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    num_words = len(word_index) + 1\n",
    "\n",
    "    embedding_matrix = np.zeros((num_words, 100))\n",
    "    unknown_words = []\n",
    "\n",
    "    for word, i in word_index.items():      \n",
    "        try:\n",
    "            embedding_matrix[i] = embeddings[word]\n",
    "        except KeyError:\n",
    "            try:\n",
    "                embedding_matrix[i] = embeddings[word.lower()]\n",
    "            except KeyError:\n",
    "                unknown_words.append(word)\n",
    "    \n",
    "    return embedding_matrix, unknown_words\n",
    "                \n",
    "glove_embeddings_matrix, _ = get_embeddings(glove_embeddings)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_words, embedding_matrix):\n",
    "    \n",
    "    model = Sequential([\n",
    "        Embedding(num_words, 100, embeddings_initializer=Constant(embedding_matrix), input_length=MAX_LEN, trainable=False),\n",
    "        SpatialDropout1D(0.2), # Her kelime için Embeddinglerin aynı boyutunu droplayabiliyor?\n",
    "        LSTM(2 ** 7, activation='tanh', recurrent_activation='sigmoid', dropout=0.1, recurrent_dropout=0.1),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train(X, y, num_words, embedding_matrix):\n",
    "    \n",
    "    N = 5    \n",
    "    skf = StratifiedKFold(n_splits=N, random_state=SEED, shuffle=True)\n",
    "    \n",
    "    oof = np.zeros((len(X_train), 1))\n",
    "    y_pred = np.zeros((len(X_test), 1))\n",
    "    \n",
    "    for fold, (trn_idx, val_idx) in enumerate(skf.split(X, y)):       \n",
    "            \n",
    "        print('\\nFold {}\\n'.format(fold))\n",
    "    \n",
    "        model = build_model(num_words, embedding_matrix)\n",
    "        model.fit(X.iloc[trn_idx], y.iloc[trn_idx], batch_size=32, epochs=10, validation_data=(X.iloc[val_idx], y.iloc[val_idx]))\n",
    "        \n",
    "        predictions = model.predict(X.iloc[val_idx])\n",
    "        oof[val_idx] = predictions\n",
    "\n",
    "        y_pred += model.predict(X_test) / N           \n",
    "            \n",
    "    return oof, y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0\n",
      "\n",
      "Train on 3565 samples, validate on 892 samples\n",
      "Epoch 1/10\n",
      "3565/3565 [==============================] - 14s 4ms/sample - loss: 0.5326 - accuracy: 0.8617 - val_loss: 0.3145 - val_accuracy: 0.8722\n",
      "Epoch 2/10\n",
      "3565/3565 [==============================] - 11s 3ms/sample - loss: 0.2074 - accuracy: 0.9316 - val_loss: 0.1334 - val_accuracy: 0.9664\n",
      "Epoch 3/10\n",
      "3565/3565 [==============================] - 11s 3ms/sample - loss: 0.1610 - accuracy: 0.9495 - val_loss: 0.1057 - val_accuracy: 0.9753\n",
      "Epoch 4/10\n",
      "3565/3565 [==============================] - 11s 3ms/sample - loss: 0.1313 - accuracy: 0.9585 - val_loss: 0.0853 - val_accuracy: 0.9742\n",
      "Epoch 5/10\n",
      "3565/3565 [==============================] - 10s 3ms/sample - loss: 0.1230 - accuracy: 0.9590 - val_loss: 0.0798 - val_accuracy: 0.9787\n",
      "Epoch 6/10\n",
      "3565/3565 [==============================] - 10s 3ms/sample - loss: 0.1189 - accuracy: 0.9607 - val_loss: 0.0785 - val_accuracy: 0.9787\n",
      "Epoch 7/10\n",
      "3565/3565 [==============================] - 11s 3ms/sample - loss: 0.1141 - accuracy: 0.9624 - val_loss: 0.0665 - val_accuracy: 0.9821\n",
      "Epoch 8/10\n",
      "3565/3565 [==============================] - 11s 3ms/sample - loss: 0.1053 - accuracy: 0.9672 - val_loss: 0.0769 - val_accuracy: 0.9787\n",
      "Epoch 9/10\n",
      "3565/3565 [==============================] - 11s 3ms/sample - loss: 0.0983 - accuracy: 0.9677 - val_loss: 0.0631 - val_accuracy: 0.9809\n",
      "Epoch 10/10\n",
      "3565/3565 [==============================] - 11s 3ms/sample - loss: 0.1046 - accuracy: 0.9655 - val_loss: 0.0653 - val_accuracy: 0.9787\n",
      "\n",
      "Fold 1\n",
      "\n",
      "Train on 3565 samples, validate on 892 samples\n",
      "Epoch 1/10\n",
      "3565/3565 [==============================] - 14s 4ms/sample - loss: 0.5437 - accuracy: 0.8617 - val_loss: 0.2296 - val_accuracy: 0.9114\n",
      "Epoch 2/10\n",
      "3565/3565 [==============================] - 11s 3ms/sample - loss: 0.1920 - accuracy: 0.9344 - val_loss: 0.1484 - val_accuracy: 0.9540\n",
      "Epoch 3/10\n",
      "3565/3565 [==============================] - 11s 3ms/sample - loss: 0.1459 - accuracy: 0.9546 - val_loss: 0.1188 - val_accuracy: 0.9652\n",
      "Epoch 4/10\n",
      "3565/3565 [==============================] - 11s 3ms/sample - loss: 0.1322 - accuracy: 0.9588 - val_loss: 0.1117 - val_accuracy: 0.9652\n",
      "Epoch 5/10\n",
      "3565/3565 [==============================] - 11s 3ms/sample - loss: 0.1187 - accuracy: 0.9619 - val_loss: 0.0941 - val_accuracy: 0.9742\n",
      "Epoch 6/10\n",
      "3565/3565 [==============================] - 11s 3ms/sample - loss: 0.1108 - accuracy: 0.9633 - val_loss: 0.0898 - val_accuracy: 0.9709\n",
      "Epoch 7/10\n",
      "3565/3565 [==============================] - 11s 3ms/sample - loss: 0.1011 - accuracy: 0.9680 - val_loss: 0.0919 - val_accuracy: 0.9720\n",
      "Epoch 8/10\n",
      "3565/3565 [==============================] - 11s 3ms/sample - loss: 0.0929 - accuracy: 0.9708 - val_loss: 0.0819 - val_accuracy: 0.9742\n",
      "Epoch 9/10\n",
      "3565/3565 [==============================] - 12s 3ms/sample - loss: 0.0927 - accuracy: 0.9691 - val_loss: 0.0977 - val_accuracy: 0.9697\n",
      "Epoch 10/10\n",
      "3565/3565 [==============================] - 15s 4ms/sample - loss: 0.0894 - accuracy: 0.9719 - val_loss: 0.0922 - val_accuracy: 0.9697\n",
      "\n",
      "Fold 2\n",
      "\n",
      "Train on 3565 samples, validate on 892 samples\n",
      "Epoch 1/10\n",
      "3565/3565 [==============================] - 14s 4ms/sample - loss: 0.5447 - accuracy: 0.8600 - val_loss: 0.2606 - val_accuracy: 0.8879\n",
      "Epoch 2/10\n",
      "3565/3565 [==============================] - 11s 3ms/sample - loss: 0.1985 - accuracy: 0.9327 - val_loss: 0.1194 - val_accuracy: 0.9686\n",
      "Epoch 3/10\n",
      "3565/3565 [==============================] - 11s 3ms/sample - loss: 0.1496 - accuracy: 0.9498 - val_loss: 0.0905 - val_accuracy: 0.9798\n",
      "Epoch 4/10\n",
      "3565/3565 [==============================] - 11s 3ms/sample - loss: 0.1400 - accuracy: 0.9506 - val_loss: 0.0838 - val_accuracy: 0.9787\n",
      "Epoch 5/10\n",
      "3565/3565 [==============================] - 11s 3ms/sample - loss: 0.1238 - accuracy: 0.9590 - val_loss: 0.0813 - val_accuracy: 0.9776\n",
      "Epoch 6/10\n",
      "3565/3565 [==============================] - 11s 3ms/sample - loss: 0.1295 - accuracy: 0.9585 - val_loss: 0.0816 - val_accuracy: 0.9765\n",
      "Epoch 7/10\n",
      "3565/3565 [==============================] - 11s 3ms/sample - loss: 0.1148 - accuracy: 0.9635 - val_loss: 0.0713 - val_accuracy: 0.9776\n",
      "Epoch 8/10\n",
      "3565/3565 [==============================] - 11s 3ms/sample - loss: 0.1048 - accuracy: 0.9658 - val_loss: 0.0792 - val_accuracy: 0.9731\n",
      "Epoch 9/10\n",
      "3565/3565 [==============================] - 12s 3ms/sample - loss: 0.1066 - accuracy: 0.9652 - val_loss: 0.0641 - val_accuracy: 0.9832\n",
      "Epoch 10/10\n",
      "3565/3565 [==============================] - 11s 3ms/sample - loss: 0.1005 - accuracy: 0.9663 - val_loss: 0.0631 - val_accuracy: 0.9843\n",
      "\n",
      "Fold 3\n",
      "\n",
      "Train on 3566 samples, validate on 891 samples\n",
      "Epoch 1/10\n",
      "3566/3566 [==============================] - 15s 4ms/sample - loss: 0.5547 - accuracy: 0.8595 - val_loss: 0.3495 - val_accuracy: 0.8664\n",
      "Epoch 2/10\n",
      "3566/3566 [==============================] - 11s 3ms/sample - loss: 0.2103 - accuracy: 0.9220 - val_loss: 0.1286 - val_accuracy: 0.9607\n",
      "Epoch 3/10\n",
      "3566/3566 [==============================] - 11s 3ms/sample - loss: 0.1637 - accuracy: 0.9450 - val_loss: 0.1157 - val_accuracy: 0.9675\n",
      "Epoch 4/10\n",
      "3566/3566 [==============================] - 11s 3ms/sample - loss: 0.1398 - accuracy: 0.9509 - val_loss: 0.1062 - val_accuracy: 0.9675\n",
      "Epoch 5/10\n",
      "3566/3566 [==============================] - 11s 3ms/sample - loss: 0.1225 - accuracy: 0.9621 - val_loss: 0.0865 - val_accuracy: 0.9753\n",
      "Epoch 6/10\n",
      "3566/3566 [==============================] - 11s 3ms/sample - loss: 0.1157 - accuracy: 0.9621 - val_loss: 0.0820 - val_accuracy: 0.9742\n",
      "Epoch 7/10\n",
      "3566/3566 [==============================] - 11s 3ms/sample - loss: 0.1073 - accuracy: 0.9649 - val_loss: 0.0813 - val_accuracy: 0.9731\n",
      "Epoch 8/10\n",
      "3566/3566 [==============================] - 11s 3ms/sample - loss: 0.1038 - accuracy: 0.9655 - val_loss: 0.0809 - val_accuracy: 0.9731\n",
      "Epoch 9/10\n",
      "3566/3566 [==============================] - 11s 3ms/sample - loss: 0.1028 - accuracy: 0.9627 - val_loss: 0.0710 - val_accuracy: 0.9787\n",
      "Epoch 10/10\n",
      "3566/3566 [==============================] - 11s 3ms/sample - loss: 0.0865 - accuracy: 0.9736 - val_loss: 0.0650 - val_accuracy: 0.9798\n",
      "\n",
      "Fold 4\n",
      "\n",
      "Train on 3567 samples, validate on 890 samples\n",
      "Epoch 1/10\n",
      "3567/3567 [==============================] - 14s 4ms/sample - loss: 0.5342 - accuracy: 0.8607 - val_loss: 0.2516 - val_accuracy: 0.8966\n",
      "Epoch 2/10\n",
      "3567/3567 [==============================] - 11s 3ms/sample - loss: 0.1945 - accuracy: 0.9344 - val_loss: 0.1447 - val_accuracy: 0.9506\n",
      "Epoch 3/10\n",
      "3567/3567 [==============================] - 11s 3ms/sample - loss: 0.1494 - accuracy: 0.9495 - val_loss: 0.1403 - val_accuracy: 0.9551\n",
      "Epoch 4/10\n",
      "3567/3567 [==============================] - 11s 3ms/sample - loss: 0.1314 - accuracy: 0.9602 - val_loss: 0.1195 - val_accuracy: 0.9618\n",
      "Epoch 5/10\n",
      "3567/3567 [==============================] - 11s 3ms/sample - loss: 0.1218 - accuracy: 0.9582 - val_loss: 0.1308 - val_accuracy: 0.9607\n",
      "Epoch 6/10\n",
      "3567/3567 [==============================] - 11s 3ms/sample - loss: 0.1278 - accuracy: 0.9571 - val_loss: 0.1050 - val_accuracy: 0.9640\n",
      "Epoch 7/10\n",
      "3567/3567 [==============================] - 11s 3ms/sample - loss: 0.1036 - accuracy: 0.9672 - val_loss: 0.1052 - val_accuracy: 0.9663\n",
      "Epoch 8/10\n",
      "3567/3567 [==============================] - 11s 3ms/sample - loss: 0.1011 - accuracy: 0.9683 - val_loss: 0.1005 - val_accuracy: 0.9685\n",
      "Epoch 9/10\n",
      "3567/3567 [==============================] - 11s 3ms/sample - loss: 0.0971 - accuracy: 0.9678 - val_loss: 0.0903 - val_accuracy: 0.9730\n",
      "Epoch 10/10\n",
      "3567/3567 [==============================] - 12s 3ms/sample - loss: 0.0986 - accuracy: 0.9650 - val_loss: 0.0824 - val_accuracy: 0.9764\n"
     ]
    }
   ],
   "source": [
    "num_words = len(tokenizer.word_index) + 1\n",
    "oof, y_pred = train(X_train, y_train, num_words, glove_embeddings_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF Precision: 0.964993 - OOF Recall: 0.937714 - OOF F1: 0.950763\n"
     ]
    }
   ],
   "source": [
    "oof_preds = np.round(oof)\n",
    "\n",
    "oof_precision = precision_score(y_train, oof_preds, average='macro')\n",
    "oof_recall = recall_score(y_train, oof_preds, average='macro')\n",
    "oof_f1 = f1_score(y_train, oof_preds, average='macro')\n",
    "\n",
    "print('OOF Precision: {:.6} - OOF Recall: {:.6} - OOF F1: {:.6}'.format(oof_precision, oof_recall, oof_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precision: 0.959358 - Test Recall: 0.938295 - Test F1: 0.948471\n"
     ]
    }
   ],
   "source": [
    "test_preds = np.round(y_pred)\n",
    "\n",
    "test_precision = precision_score(y_test, test_preds, average='macro')\n",
    "test_recall = recall_score(y_test, test_preds, average='macro')\n",
    "test_f1 = f1_score(y_test, test_preds, average='macro')\n",
    "\n",
    "print('Test Precision: {:.6} - Test Recall: {:.6} - Test F1: {:.6}'.format(test_precision, test_recall, test_f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
